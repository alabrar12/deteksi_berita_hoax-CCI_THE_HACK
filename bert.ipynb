{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26dc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0606be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_HOAX_TAGS = ['berita', 'fakta', 'klarifikasi', 'benar', 'cek fakta']\n",
    "def contains_non_hoax_tag(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    # Deteksi tag di awal teks dengan [] atau ()\n",
    "    match = re.match(r'[\\[\\(]\\s*([^\\]\\)]+?)\\s*[\\]\\)]', text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        tag = match.group(1).strip().lower()\n",
    "        return tag in NON_HOAX_TAGS\n",
    "    return False\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # Hapus tag di awal seperti [HOAX], (SALAH), dll\n",
    "    text = re.sub(r'^[\\[\\(]\\s*[^)\\]]+\\s*[\\)\\]]', '', text)\n",
    "    # Hapus link\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Hapus karakter selain huruf dan spasi\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # Hilangkan spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    # Preprocess dataframe with text cleaning\n",
    "    df = df[~df['judul'].apply(contains_non_hoax_tag)].copy()\n",
    "    df['text'] = df['judul'].fillna('') + ' ' + df['summary'].fillna('')\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    df = df[df['clean_text'].str.strip() != '']\n",
    "    df = df.dropna(subset=['clean_text', 'hoax'])\n",
    "    return df[['clean_text', 'hoax']]\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    # Load all datasets and combine them\n",
    "    sum_paths = {\n",
    "        \"cnn\": 'D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK/Summarized/Summarized_CNN.csv',\n",
    "        \"detik\": 'D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK/Summarized/Summarized_Detik.csv',\n",
    "        \"kompas\": 'D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK/Summarized/Summarized_Kompas.csv',\n",
    "        \"tbh\": 'D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK/Summarized/Summarized_TurnBackHoax.csv'\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for source, path in sum_paths.items():\n",
    "        print(f\"Loading {source} dataset...\")\n",
    "        df = pd.read_csv(path)\n",
    "        df_processed = preprocess_dataframe(df)\n",
    "        \n",
    "        # Set labels: CNN, Detik, Kompas = 0 (Non-Hoax), TBH = 1 (Hoax)\n",
    "        if source != 'tbh':\n",
    "            df_processed['hoax'] = 0\n",
    "        else:\n",
    "            df_processed['hoax'] = 1\n",
    "            \n",
    "        print(f\"{source}: {len(df_processed)} samples\")\n",
    "        all_data.append(df_processed)\n",
    "\n",
    "        if source == 'tbh':\n",
    "            print(\"\\nContoh baris TBH sebelum preprocessing:\")\n",
    "            print(df.head(3)[['judul', 'summary']])\n",
    "            print(\"\\nContoh baris TBH setelah preprocessing:\")\n",
    "            print(df_processed.head(3))\n",
    "\n",
    "    \n",
    "    # Combine all datasets\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nTotal combined dataset: {len(combined_df)} samples\")\n",
    "    print(f\"Class distribution:\\n{combined_df['hoax'].value_counts()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d539f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoaxDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08b65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTHoaxClassifier(nn.Module):\n",
    "    def __init__(self, model_name='indobenchmark/indobert-base-p1', num_classes=2, freeze_layers=8):\n",
    "        super(IndoBERTHoaxClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze first 8 layers (layers 1-8), unfreeze layers 9-12\n",
    "        for i, layer in enumerate(self.bert.encoder.layer):\n",
    "            if i < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4042f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar dengan loss info\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            actual_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return predictions, actual_labels, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f841b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # ROC AUC Score\n",
    "    y_probs_class1 = [prob[1] for prob in y_probs]  # Probability for class 1 (hoax)\n",
    "    roc_auc = roc_auc_score(y_true, y_probs_class1)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Hoax', 'Hoax'], \n",
    "                yticklabels=['Non-Hoax', 'Hoax'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_probs):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    y_probs_class1 = [prob[1] for prob in y_probs]\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs_class1)\n",
    "    roc_auc = roc_auc_score(y_true, y_probs_class1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fcc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_and_prepare_data()\n",
    "    \n",
    "    # Split data (70/15/15)\n",
    "    X = df['clean_text'].values\n",
    "    y = df['hoax'].values\n",
    "    \n",
    "    # First split: 70% train, 30% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: 15% val, 15% test from the 30% temp\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    model_name = 'indobenchmark/indobert-base-p1'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = IndoBERTHoaxClassifier(model_name=model_name, freeze_layers=8).to(device)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_dataset = HoaxDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = HoaxDataset(X_val, y_val, tokenizer)\n",
    "    test_dataset = HoaxDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    epochs = 5\n",
    "    learning_rate = 1e-5\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_preds, val_true, val_probs = eval_model(model, val_loader, device)\n",
    "        val_metrics = calculate_metrics(val_true, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1-Score: {val_metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_preds, test_true, test_probs = eval_model(model, test_loader, device)\n",
    "    test_metrics = calculate_metrics(test_true, test_preds, test_probs)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Test Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "    print(f\"Test ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_true, test_preds, target_names=['Non-Hoax', 'Hoax']))\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_confusion_matrix(test_true, test_preds)\n",
    "    plot_roc_curve(test_true, test_probs)\n",
    "    \n",
    "    # Save model and tokenizer ke Kaggle output directory\n",
    "    import os\n",
    "    output_dir = '/kaggle/working/hoax_detector_model'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    torch.save(model.state_dict(), f'{output_dir}/hoax_detector_model.pth')\n",
    "    tokenizer.save_pretrained(f'{output_dir}/tokenizer')\n",
    "    \n",
    "    # Save training metrics juga\n",
    "    import pickle\n",
    "    with open(f'{output_dir}/training_metrics.pkl', 'wb') as f:\n",
    "        pickle.dump(test_metrics, f)\n",
    "    \n",
    "    print(f\"\\nModel and tokenizer saved to {output_dir}\")\n",
    "    print(\"PENTING: Setelah training selesai, commit notebook dan create dataset dari output!\")\n",
    "    \n",
    "    return model, tokenizer, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "210e8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoaxDetector:\n",
    "    def __init__(self, model_path=None, tokenizer_path=None):\n",
    "        \"\"\"\n",
    "        Jika model_path None, akan coba load dari dataset Kaggle\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Jika tidak ada path, coba load dari dataset\n",
    "        if model_path is None:\n",
    "            # Ganti dengan path dataset Anda nanti\n",
    "            base_path = 'D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK'  # <-- GANTI INI\n",
    "            model_path = f'{base_path}/bert_model_state_dict.pth'\n",
    "            tokenizer_path = f'{base_path}/tokenizer'\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            \n",
    "            # Load model\n",
    "            self.model = IndoBERTHoaxClassifier()\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"✅ Model loaded successfully from {model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model: {e}\")\n",
    "            print(\"Pastikan path dataset sudah benar!\")\n",
    "    \n",
    "    def predict(self, judul, isi_berita):\n",
    "        \"\"\"\n",
    "        Predict whether news is hoax or not\n",
    "        Args:\n",
    "            judul: News title\n",
    "            isi_berita: News content\n",
    "        Returns:\n",
    "            Dict with prediction, confidence, and probabilities\n",
    "        \"\"\"\n",
    "        # Combine title and content\n",
    "        text = f\"{judul} {isi_berita}\"\n",
    "        clean_text = clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            clean_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        # Format results\n",
    "        prob_non_hoax = probabilities[0][0].item()\n",
    "        prob_hoax = probabilities[0][1].item()\n",
    "        \n",
    "        prediction = \"HOAX\" if predicted_class == 1 else \"NON-HOAX\"\n",
    "        \n",
    "        result = {\n",
    "            'prediction': prediction,\n",
    "            'confidence': f\"{confidence*100:.1f}%\",\n",
    "            'probabilities': {\n",
    "                'Non-Hoax': f\"{prob_non_hoax:.3f}\",\n",
    "                'Hoax': f\"{prob_hoax:.3f}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fd8805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_article():\n",
    "    \"\"\"Example function to test new articles\"\"\"\n",
    "    \n",
    "    # Initialize detector\n",
    "    detector = HoaxDetector()\n",
    "    \n",
    "    # Example Hoaks\n",
    "    judul1 = \"Kemenkes Wajibkan Penumpang Pesawat Tervaksinasi TBC\"\n",
    "    isi1 = \"\"\"Kemenkes: Semua penumpang yang akan naik pesawat agar sudah divaksin TBC dan menunjukan surat vaksin. tujuannya untuk mencegah penyebaran lewat udara. Budi Gunadi Sadikin, Menkes\"\"\"\n",
    "    \n",
    "    result1 = detector.predict(judul1, isi1)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CONTOH TESTING BERITA BARU\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Judul: {judul1}\")\n",
    "    print(f\"Prediksi: {result1['prediction']}\")\n",
    "    print(f\"Confidence: {result1['confidence']}\")\n",
    "    print(f\"Probabilitas Non-Hoax: {result1['probabilities']['Non-Hoax']}\")\n",
    "    print(f\"Probabilitas Hoax: {result1['probabilities']['Hoax']}\")\n",
    "    \n",
    "    # Example 2\n",
    "    judul2 = \"Tanpa Messi dan Mbappe, PSG Bisa Juara Liga Champions\"\n",
    "    isi2 = \"\"\"Paris Saint-Germain (PSG) akhirnya berhasil meraih gelar Liga Champions pertama mereka di musim 2024/2025, meski tanpa kehadiran dua bintang besar yang sebelumnya membela klub itu yaitu Lionel Messi dan Kylian Mbappe. Musim ini menjadi titik tolak bagi klub ibu kota Prancis itu. Dengan hengkangnya Messi pada musim panas 2023 lalu ke Inter Miami dan Mbappe yang memilih pindah ke Real Madrid pada Juli 2024, banyak yang meragukan kemampuan PSG untuk bersaing di kompetisi level tertinggi Eropa. Namun, justru di luar dugaan, PSG tampil lebih solid dan kolektif, mengesampingkan ketergantungan pada satu atau dua pemain berlabel bintang besar.\"\"\"\n",
    "    \n",
    "    result2 = detector.predict(judul2, isi2)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Judul: {judul2}\")\n",
    "    print(f\"Prediksi: {result2['prediction']}\")\n",
    "    print(f\"Confidence: {result2['confidence']}\")\n",
    "    print(f\"Probabilitas Non-Hoax: {result2['probabilities']['Non-Hoax']}\")\n",
    "    print(f\"Probabilitas Hoax: {result2['probabilities']['Hoax']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe25e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and preparing data...\n",
      "Loading cnn dataset...\n",
      "cnn: 4216 samples\n",
      "Loading detik dataset...\n",
      "detik: 4214 samples\n",
      "Loading kompas dataset...\n",
      "kompas: 4216 samples\n",
      "Loading tbh dataset...\n",
      "tbh: 12012 samples\n",
      "\n",
      "Contoh baris TBH sebelum preprocessing:\n",
      "                                               judul  \\\n",
      "0  [PENIPUAN] Tautan “New Gebyar Program Bank BCA...   \n",
      "1  [SALAH] Video “Ada Bangkai Kereta di Tebing, A...   \n",
      "2  [SALAH] Jokowi Pakai Rp38,5 Triliun Dana Haji ...   \n",
      "\n",
      "                                             summary  \n",
      "0  untuk seluruh Nasabah Bank BCA . unggahan ters...  \n",
      "1  bangkai gerbong kereta bekas kecelakaan mangkr...  \n",
      "2  pemeriksaan Fakta Tim Pemeriksa Fakta MAFINDO ...  \n",
      "\n",
      "Contoh baris TBH setelah preprocessing:\n",
      "                                          clean_text  hoax\n",
      "0  tautan new gebyar program bank bca tahun untuk...     1\n",
      "1  video ada bangkai kereta di tebing akibat jemb...     1\n",
      "2  jokowi pakai rp triliun dana haji rakyat tak d...     1\n",
      "\n",
      "Total combined dataset: 24658 samples\n",
      "Class distribution:\n",
      "hoax\n",
      "0    12646\n",
      "1    12012\n",
      "Name: count, dtype: int64\n",
      "Train: 17260, Val: 3699, Test: 3699\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d68639883e640cab9e45a36836973fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run training\n",
    "    model, tokenizer, metrics = main_training()\n",
    "    \n",
    "    # Test with new articles\n",
    "    # test_new_article()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCRIPT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93c6e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully from D:/Semester 4/CCI/The Hack 2025/deteksi_berita_hoax-CCI_THE_HACK/bert_model_state_dict.pth\n",
      "============================================================\n",
      "CONTOH TESTING BERITA BARU\n",
      "============================================================\n",
      "Judul: Kemenkes Wajibkan Penumpang Pesawat Tervaksinasi TBC\n",
      "Prediksi: HOAX\n",
      "Confidence: 66.5%\n",
      "Probabilitas Non-Hoax: 0.335\n",
      "Probabilitas Hoax: 0.665\n",
      "\n",
      "------------------------------------------------------------\n",
      "Judul: Tanpa Messi dan Mbappe, PSG Bisa Juara Liga Champions\n",
      "Prediksi: NON-HOAX\n",
      "Confidence: 99.7%\n",
      "Probabilitas Non-Hoax: 0.997\n",
      "Probabilitas Hoax: 0.003\n"
     ]
    }
   ],
   "source": [
    "test_new_article()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
